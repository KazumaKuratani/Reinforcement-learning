# DQN実装の全体設計
## ➀ReplayBuffer(経験再生バッファ)
・過去の経験を保存しておくためのデータ構造  
・データの相関を断ち切り、学習を安定させる   
・最大容量を超えたら古いものから捨てるキュー構造

## ➁Q-Network(モデル)
入力：状態s　　出力：各行動aの価値Q(s,a)  
・現在の学習用ネットワーク(メインネットワーク)とターゲットネットワークを準備する。  
・Q値はニューラルネットワークを通してのみ取り出せる。（∵テーブルとして保存できない）  
・重みの更新を行うとQ(s,a)とQ(s',a')どちらも動いてしまうが、正解ラベルが動いてしまうと困るからターゲットネットワークに"重み"のみ保存しとく  
・ターゲットネットワークに保存されている"少し古い重み"を使ってQ(s',a')を算出する

## ➂Agent(エージェント)
 ・ε-greedy法による行動選択、バッファからのサンプリング、損失計算（Lossの算出）、重みの更新。

## ➃TaskManeger/MainLoop(メインループ)
・エピソードの繰り返しを管理する部分です。
