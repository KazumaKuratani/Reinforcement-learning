# DDQN理論
## ➀そもそもQ学習とは
「ブーストトラップ法」と「ベルマン最適方程式」を利用して「"今"の行動価値関数」を「得られた"実際"の報酬」と「次の状態の"最適"行動価値関数」で近似する。  


ここで近似する理由は、Q学習はモンテカルロ法と違い、最後までの行動を観測せずにその時に実際に得られた報酬と次の状態の予測値を使って行動価値関数を更新する。こうすることによって最後まで（つまり1エピソードが終わるまで）待たずに更新できるためモンテカルロ法に比べ更新が速くなる。ここでベルマン最適方程式とブーストトラップ法の式を思い出そう。下記のようになる。  
$$Q_n = r + \gamma \max_{a'} Q(s', a')$$
$$V_n = V_{n-1} + \alpha (S_n - V_{n-1})$$
$$S_n=Target$$
最後にTargetを設定しているがしっかりここで意識したいのがVn-1は「今までの"見積もり"」を使って、Targetは「"実際"に得られた報酬」と「次の状態の見積もり」を使うということ。  
ここで1つ疑問なのが「なぜベルマン最適方程式を使って近似するのか」ということ。  
→この問いに答えるために重要なのが学習途中においての真の値（信用できる値）はどれかということ。この答えは唯一「得られた"実際"の報酬r」のみである。この真実が拡散されることによって最終的に真の最適行動価値関数が得られるだろうという話。

そしてもう1つここで重要なのが「ベルマン最適方程式」が成り立つ前提条件は自分が最適方策を選んでいるということ。つまり自分が<font color="Red">**最強**</font>（最適方策を選べる）という<font color="Red">**前提条件**</font>がある。しかし学習途中においては最強ではない。そのため近似するには少し問題が生じる。ここで1つの妥協策としてDDQNがある。（まずQ学習においてベルマン最適方程式を利用していることが<font color="Red">**妥協案**</font>だということを強く意識する必要がある）

## ➁DQNとは
### ニューラルネットワーク×Q学習
今までのモンテカルロ法やQ学習は基本的にはテーブル形式で行動価値関数を保存する。つまりある"状態"と"行動"においての価値関数はペアの形で"保存"されている。しかしテーブル形式で保存できない場合は大いにある。例えば将棋を例にとっても局面の数だけを見ても約$10^{71}$ 通りある。これは宇宙に存在する原子の数（約 $10^{80}$ 個）に近いというとんでもない数字になっていて、今の標準の1TBのパソコンでは約 $10^{12}$通りしか保存できないことを考えてもテーブル形式ですべての行動価値を保存するのは不可能であることが分かる。ここでテーブル形式で保存するのは不可能だが、妥協策として行動価値を近似できる関数があればいいよねっていう話になる。ここで近似する関数としてニューラルネットワークを利用する。  
つまりまとめると、テーブル形式で保存できない行動価値関数をニューラルネットワークで近似することでどんな環境においても行動価値関数を算出できるようにしようねという話。(だから保存するのは、テーブル形式のペアで保存された行動価値ではなく、「近似する関数」＝「ニューラルネットワーク」を保存する)
#### *問題点1→*ニューラルネットワークを利用するにあたって正解ラベルをどう定義するかという話
まずQ学習において正解ラベルは何になるかということ。
ここで思い出してほしいのがベルマン最適方程式である。ベルマン最適方程式がほとんど成り立つときはそれはほぼ最適な行動価値関数（つまり最適な方策を選べるということ）であるということである。数式で言うと$$S_n \approx r + \gamma \max_{a'} Q(s', a')$$がほとんど成り立つということである。これより正解ラベルは
$$max_{a'} Q(s', a')$$
を利用する。(rとγは決められた値になるからとりあえず無視)  
しかし、問題になるのがQ(s',a')はニューラルネットワークを通してのみ取り出せる（決定できる）ということ。これの何が問題なのかというともともと何をしたかったかという原点に戻ると、良いQ(s,a)を導き出せる(近似できる)ような関数を見つけようね、で正解ラベルはQ(s',a')を利用しようねということだったと思うが、この正解ラベルはQ(s,a)と同じ関数を通して導き出すわけだからQ(s,a)をQ(s',a')を含む正解ラベルに近似する良いパラメータを更新するとQ(s',a')も同じように更新されてしまい、一生追い付けないことになる。そのためこの問題を解決するために2つのニューラルネットワークを用意して1つは更新用、そしてもう1つは正解ラベルを取り出すためのニューラルネットワークを作る。これらはそれぞれメインネットワークとターゲットネットワークと呼ぶ。正解ラベルが固定され、とりあえず正解ラベルが動いてしまう問題が解決された。しかし、ここでも少々問題があるがそれを解決するのがDDQNである。簡単に問題だけを言うと正解ラベルをmaxQ(s',a')を利用しているということ。これはベルマン最適方程式がほとんど成り立つ（自分は最適方策を選んでいる）であろうという仮定から利用しているが、ここが問題になる。


#### *問題点2→*入力ラベルをどう入れるかという話
入力ラベルの話についてMNISTを例にとって考える。MNISTは0~9の数字のベクトル(28*28の長さ)を入力としてどんなデータが入れられても正解できるような関数を作る。ここで入力するデータは10,000枚だとすると入力データはランダムに入れるよね（0~9の数字をランダムに入力データに入れる）。しかし最初の1,00枚は0で次の1,000枚は1で...最後の1,000枚は9を入力データとしたとき、9だけは上手く判別できるが0,1...8は上手く判別する表現力を得れないことは自明だろう。これから分かることはニューラルネットワークの入力データはランダムに選ばれないといけないということ。  
これを踏まえてDQNの入力データについて考えると、DQNの性質上（というか強化学習の性質上）得られる入力データ（つまり状態s）は同じようなものが連続する。なぜなら得られる状態はs1,s2...sn-1,snと順番に得られてしまいこの順に入力データに入れると最後のsn辺りの状態から行動価値関数を近似する表現力は得られるが、s1,s2辺りの表現力はないよね。
だからこれを解決しようという話。  
ここで使われるのが**経験再生**である。この**経験再生**については簡単だからここでは説明しない。

## ➂DDQN   自意識過剰を超えて
DQNとDDQNの違いは"行動価値関数"の"更新"の違いにある。もう少しかみ砕いて説明すると、DQNの行動価値関数の正解ラベルは自分が最適方策を選んでいるという前提条件があるから成り立つことである。で少々ここで問題なのがこの"**自意識過剰**"であること。もし最初にこの行動は良いだろうと間違って学習した場合、それ以降その行動を信じて動き回ってしまう可能性がある。(真実の拡散によってこの問題はそれなりに解消はされていくが...)  
これをより良い最適方策の選び方があればいいよねという話になる。そこで利用するのがメインネットワークとターゲットネットワークの利用である。結論から言うとDDQNはターゲットネットワークで最適な行動を選び、メインネットワークでその選んだ行動を評価する。数式ベースでみると下記のようになる。 

*DQNのターゲット*:$$y_t^{\text{DQN}} = r + \gamma \max_{a'} Q(s', a'; \theta^-)$$※ $\theta^-$ はターゲットネットワークのパラメータ。選択と評価の両方に $\theta^-$ を使用。  


*DDQNのターゲット*:$$y_t^{\text{DDQN}} = r + \gamma Q(s', \text{argmax}_{a'} Q(s', a'; \theta); \theta^-)$$※ $\text{argmax}$ 内の $\theta$ はメインネットワーク。外側の $\theta^-$ はターゲットネットワーク。

DQNは、ターゲットネットワークから最適行動を選び、*ターゲットネットワーク*でその行動の最適価値を算出(評価)する。一方で  
DDQNは、ターゲットネットワークから最適行動を選び、*メインネットワーク*でその行動の最適価値を算出(評価)する。  
こうすることによって、行動自体は自分を信じて決定するが、評価は他の人にしてもらうことで、客観的に評価される。そのことによって自分が最強であるという思い込みを軽減できる。これだけでは理解はできても納得できない(理論が弱い)と思うため統計学的にこれが正しいことを簡単に証明する。  
### 統計学的にDDQNがDQNより優れている理由
まず復習するとQ(s,a)は期待値計算により求められる。(下記)
$$Q^{\pi}(s, a) = E_{\pi} [G_t \mid S_t = s, A_t = a]$$
$$Q^*(s, a) = E[r + \gamma \max_{a'} Q^*(s', a')]$$
また学習中のターゲットネットワークは誤差が含まれる。数式で表すと下記のようになる。
$$Q(s', a'; \theta) = Q^*(s', a') + \epsilon_{a'}$$
 ＊$\epsilon_{a'}$ は平均 0 のランダムな誤差。  
 そして誤差（ノイズ）を含む値の最大値を取ると、その期待値は真の最大値よりも大きくなる。

$$E[\max_{a'} Q(s', a')] \ge \max_{a'} E[Q(s', a')]$$

これを解消できるのがDDQNであるという話。理論については上で述べているのでそれを参照で。